# ----------------------------------------------------------------------------
# Copyright 2014 Nervana Systems Inc.  All rights reserved.
# ----------------------------------------------------------------------------
# based on maxout (goodfellow 2013) and NiN (Lin 2013) 
# we have a stack of 3 NiN layers, with dropout on the first 2 and max-pooling on all.
# instead of an FC layer this is followed by global average pooling.


[data]
type=data
dataIdx=0

[labvec]
type=data
dataIdx=1

[conv1a]
type=conv
inputs=data
channels=3
filters=192
padding=2
stride=1
filterSize=5
initW=0.05
sumWidth=4
sharedBiases=1
gpu=0,1

# 1x1 convolutions
[conv1b]
type=conv
inputs=conv1a
channels=192
filters=160
padding=0
stride=1
filterSize=1
initW=0.05
sumWidth=4
sharedBiases=1

[conv1c]
type=conv
inputs=conv1b
channels=160
filters=96
padding=0
stride=1
filterSize=1
initW=0.05
sumWidth=4
sharedBiases=1

[pool1]
type=pool
pool=max
inputs=conv1c
sizeX=3
stride=2
channels=96
neuron=relu

# dropout after pooling
[dropout1]
type=dropout2
inputs=pool1

[conv2a]
type=conv
inputs=dropout1
filters=192
padding=2
stride=1
filterSize=5
channels=96
initW=0.05
initB=1
sumWidth=3
sharedBiases=1
neuron=relu

[conv2b]
type=conv
inputs=conv2a
filters=192
padding=0
stride=1
filterSize=1
channels=192
initW=0.05
initB=1
sumWidth=3
sharedBiases=1
neuron=relu

[conv2c]
type=conv
inputs=conv2b
filters=192
padding=0
stride=1
filterSize=1
channels=192
initW=0.05
initB=1
sumWidth=3
sharedBiases=1
neuron=relu

[pool2]
type=pool
pool=max
inputs=conv2c
sizeX=3
stride=2
channels=192

[dropout2]
type=dropout2
inputs=pool2

[conv3a]
# already reduce filters to 10 here?
type=conv
inputs=dropout2
filters=192
padding=1
stride=1
filterSize=3
channels=192
initW=0.05
initB=1
sumWidth=3
sharedBiases=1
neuron=relu

[conv3b]
type=conv
inputs=conv3a
# want 10 filters here but minimum is 32
filters=192
padding=0
stride=1
filterSize=1
channels=192
initW=0.05
initB=1
sumWidth=3
sharedBiases=1
neuron=relu
[conv3c]
type=conv
inputs=conv3b
# want 10 filters here but minimum is 32
filters=32
padding=0
stride=1
filterSize=1
channels=192
initW=0.05
initB=1
sumWidth=3
sharedBiases=1
neuron=relu
# no!
#[dropout3]
#type=dropout2
#inputs=conv3b

# I dont think it makes sense to have two consecutive poolings!
# this pooling is from the maxout network.
# it's different from avearge though...
# [pool3] 
# type=pool
# pool=max
# inputs=conv3b
# sizeX=3
# stride=2
# channels=32

# how to get to 10 feature maps 

# average pooling
[poolout]
type=pool
pool=avg
inputs=conv3c
# last layer is a 2x2 output that we pool now
sizeX=8
stride=1
channels=32
neuron=relu

# FC to bring the size down
#[fc10]
#type=fc
#outputs=10
#inputs=poolout
#initW=0.01
#initB=-7

# instead of FC, make a layer that's not learned and goes from 16 to 10.
[downsize]
type=fc
inputs=poolout
outputs=10
# we give "params", automatically sends name, idx, shape.
initW = 0
initWFunc=winitfile.makew(0.5,3)
initB = 0

[probs]
type=softmax
inputs=downsize

[logprob]
type=cost.logreg
inputs=labvec,probs
gpu=0,1

