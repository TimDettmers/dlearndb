# ----------------------------------------------------------------------------
# Copyright 2014 Nervana Systems Inc.  All rights reserved.
# ----------------------------------------------------------------------------
[data]
type=data
dataIdx=0

[filtering]
# input is 24x24x3
# filters and channels should be divisible by 64
type=local
inputs=data
# can the 3 channels be faked to be 16?
channels=3
# minimum of 16 filters, WTF!
filters=16
# use padding so the defiltering comes out with the same size
padding=0
stride=1
filterSize=11
# rgb input channels
neuron=ident
gpu=0
initW=0.01

# second weight layer, has 3600=60*60=16*15*15
[recon]
type=fc
inputs=filtering
channels = 16 # gets ignored, WTF?!
# must be divisible by 16,
# filters=3 # 16 # 1728 inputs
outputs = 1728 
# 3*24*24=1728, 3*14*14=588
neuron=ident
initW=0.01




# #set up for pooling - produces 24x24x16 
[square]
type=neuron
inputs=filtering
neuron = square

# # pooling layer that squares --produces 16x21x21 outputs
[l2pool]
type=pool
inputs=square
# there is no l2, only "max" and "avg"
pool=avg
sizeX=4
stride=1
# pooling is per channel, i.e. equals filtering
channels=16
neuron=ident

[sqrt]
inputs=l2pool
type=neuron
neuron = sqrt

# # dont sum up the norms into a single output
# # compute extra sqrt and let the sum of squares take care of it. 

[sqrt2]
inputs=sqrt
type=neuron
neuron = sqrt




# -------------------------
# set up for cost layer
[diff]
#data is 3x24x24, recon is 16x24x24
inputs=recon,data
type=eltsum


[concat]
type=concat
inputs=diff,sqrt2

# cost layer - sum of squares
# this layer seems to die when it tries to reshape a matrix that is not owned
# it can also produce negative ouputs which is quite strange. 
# HERE THE INPUT CAN BE DIFF OR CONCAT TO ENABLE / DISABLE THE SPARSENESS
[sqpiff]
type=cost.sum2
#inputs=diff
inputs=concat
# -------------------------
