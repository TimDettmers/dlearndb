[data]
type=data
dataIdx=0

[labels]
type=data
dataIdx=1

[conv1]
type=conv
inputs=data
channels=3
filters=64
# need the padding?
padding=2
stride=1       
# down to 1
filterSize=5   
# down to 5
initW=0.0001
sumWidth=4
sharedBiases=1
gpu=0
# need to specify?
neuron=relu 
# relu

[pool1]
type=pool
pool=max
inputs=conv1
sizeX=3
stride=2
channels=64
# i don't thin pool should relu!
#neuron=relu
# defaults
start=0
outputsX=0

[rnorm1]      
# switched order with pool1
type=cmrnorm
inputs=pool1
channels=64
# size from 5 up to 9
size=9

[conv2]
type=conv
inputs=rnorm1
filters=64
padding=2
stride=1
filterSize=5
channels=64
initW=0.01
# not init b?
#initB=1
# not sure what this does?
sumWidth=2
sharedBiases=1
# defaults to "ident" 
neuron=relu 
# relu

[rnorm2]
type=cmrnorm
inputs=conv2
channels=64
# kriz uses 9, I think 5 is good?
size=9

[pool2]
type=pool
pool=max
inputs=rnorm2
sizeX=3
stride=2
channels=64
# neede dor not?
outputsX=0

# # I want to use a big FC dropout layer instead of two local layers. 
# [fc100]
# type=fc
# inputs=pool2
# # the input to this is only 5x5 after the 2nd pooling, don't have lots of neurons. 
# outputs=100
# initW=0.01
# initB=1
# neuron=relu

# [dropout1]
# type=dropout2
# inputs=fc100

# --------------------- original ----------------

[local3]
type=local
inputs=pool2
filters=64
padding=1
stride=1
filterSize=3
channels=64
neuron=relu
initW=0.04

[local4]
type=local
inputs=local3
filters=32
padding=1
stride=1
filterSize=3
channels=64
neuron=relu
initW=0.04
# -------------------------------------------------


[fc10]
type=fc
outputs=10
inputs=local4
initW=0.01
#initB=-7

[probs]
type=softmax
inputs=fc10

[logprob]
type=cost.logreg
inputs=labels,probs
gpu=0
